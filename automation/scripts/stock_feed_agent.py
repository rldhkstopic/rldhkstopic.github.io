#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì£¼ì‹ ë‰´ìŠ¤ í”¼ë“œ ìˆ˜ì§‘ Agent
RSS í”¼ë“œì™€ SNS ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ì—¬ stock_feed.jsonì„ ì—…ë°ì´íŠ¸í•œë‹¤.
"""

import os
import json
import hashlib
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Set, Optional
from zoneinfo import ZoneInfo
from email.utils import parsedate_to_datetime
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜
ECONOMIC_NEWS_RSS_FEEDS = os.getenv("ECONOMIC_NEWS_RSS_FEEDS", "").strip()
STOCK_FEED_JSON_PATH = Path("assets/data/stock_feed.json")
MAX_ITEMS = 200

# ê¸°ë³¸ RSS í”¼ë“œ
DEFAULT_FEEDS = [
    "https://feeds.bloomberg.com/markets/news.rss",
    "https://feeds.bloomberg.com/technology/news.rss",
    "https://feeds.bloomberg.com/politics/news.rss",
]

# ê´€ì‹¬ ì¢…ëª© í‹°ì»¤ (í•œêµ­: 6ìë¦¬, ë¯¸êµ­: ëŒ€ë¬¸ì)
WATCHLIST_TICKERS = ["SOFI"]


def generate_item_id(url: str, timestamp: str) -> str:
    """URLê³¼ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ê³ ìœ  ID ìƒì„±"""
    content = f"{url}|{timestamp}"
    return hashlib.md5(content.encode()).hexdigest()


def extract_tickers(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ í‹°ì»¤ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­)"""
    found = []
    text_upper = text.upper()
    
    for ticker in WATCHLIST_TICKERS:
        if ticker in text_upper:
            found.append(ticker)
    
    return found


def categorize_item(item: Dict) -> str:
    """ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
    tickers = item.get("related_tickers", [])
    if tickers:
        return "WATCHLIST"
    
    content_lower = item.get("content", "").lower()
    if any(keyword in content_lower for keyword in ["ê¸´ê¸‰", "ì†ë³´", "ì¤‘ìš”", "breaking"]):
        return "MAJOR"
    
    return "MARKET"


def determine_sentiment(content: str) -> Optional[str]:
    """ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
    content_lower = content.lower()
    
    positive_keywords = ["ìƒìŠ¹", "ì¦ê°€", "ì„±ì¥", "ê¸ì •", "í˜¸ì¬", "ìƒí–¥", "ê°œì„ "]
    negative_keywords = ["í•˜ë½", "ê°ì†Œ", "ë¶€ì •", "ì•…ì¬", "í•˜í–¥", "ì•…í™”", "ìš°ë ¤"]
    
    pos_count = sum(1 for kw in positive_keywords if kw in content_lower)
    neg_count = sum(1 for kw in negative_keywords if kw in content_lower)
    
    if pos_count > neg_count:
        return "POSITIVE"
    elif neg_count > pos_count:
        return "NEGATIVE"
    else:
        return "NEUTRAL"


def collect_rss_news() -> List[Dict]:
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    feed_urls = DEFAULT_FEEDS
    if ECONOMIC_NEWS_RSS_FEEDS:
        feed_urls = [u.strip() for u in ECONOMIC_NEWS_RSS_FEEDS.split(",") if u.strip()]
    
    all_items = []
    tz = ZoneInfo("Asia/Seoul")
    cutoff_time = datetime.now(tz) - timedelta(hours=24)
    
    for url in feed_urls:
        try:
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
            
            root = ET.fromstring(resp.content)
            channel = root.find("channel")
            if channel is None:
                continue
            
            for item in channel.findall("item"):
                title_el = item.find("title")
                link_el = item.find("link")
                pub_el = item.find("pubDate")
                desc_el = item.find("description")
                
                title = (title_el.text or "").strip() if title_el is not None else ""
                link = (link_el.text or "").strip() if link_el is not None else ""
                pub = (pub_el.text or "").strip() if pub_el is not None else ""
                desc = (desc_el.text or "").strip() if desc_el is not None else ""
                
                if not title or not link:
                    continue
                
                # ì‹œê°„ íŒŒì‹±
                try:
                    dt = parsedate_to_datetime(pub)
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=ZoneInfo("UTC"))
                    dt_kst = dt.astimezone(tz)
                    
                    if dt_kst < cutoff_time:
                        continue
                except Exception:
                    continue
                
                # ì¶œì²˜ ì¶”ì¶œ
                source_name = "Bloomberg"
                if "bloomberg" in url.lower():
                    source_name = "Bloomberg"
                elif "hankyung" in url.lower():
                    source_name = "í•œêµ­ê²½ì œ"
                elif "mk" in url.lower() or "ë§¤ê²½" in url.lower():
                    source_name = "ë§¤ì¼ê²½ì œ"
                
                # í‹°ì»¤ ì¶”ì¶œ
                related_tickers = extract_tickers(title + " " + desc)
                
                # ì•„ì´í…œ ìƒì„±
                feed_item = {
                    "id": generate_item_id(link, dt_kst.isoformat()),
                    "timestamp": dt_kst.isoformat(),
                    "source_type": "NEWS",
                    "source_name": source_name,
                    "category": "MARKET",  # ë‚˜ì¤‘ì— categorize_itemë¡œ ì—…ë°ì´íŠ¸
                    "related_tickers": related_tickers,
                    "content": title + (" - " + desc[:200] if desc else ""),
                    "url": link,
                    "sentiment": None,
                }
                
                feed_item["category"] = categorize_item(feed_item)
                feed_item["sentiment"] = determine_sentiment(feed_item["content"])
                
                all_items.append(feed_item)
        
        except Exception as e:
            print(f"[WARN] RSS ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")
            continue
    
    return all_items


def collect_reddit_posts() -> List[Dict]:
    """Redditì—ì„œ ì£¼ì‹ ê´€ë ¨ ê²Œì‹œë¬¼ ìˆ˜ì§‘ (ê°„ë‹¨í•œ ì›¹ ìŠ¤í¬ë˜í•‘)"""
    # Reddit RSS í”¼ë“œ ì‚¬ìš© (ê³µê°œ API)
    reddit_feeds = [
        "https://www.reddit.com/r/stocks/hot/.rss",
        "https://www.reddit.com/r/investing/hot/.rss",
        "https://www.reddit.com/r/koreastock/hot/.rss",
    ]
    
    all_items = []
    tz = ZoneInfo("Asia/Seoul")
    cutoff_time = datetime.now(tz) - timedelta(hours=24)
    
    for url in reddit_feeds:
        try:
            headers = {"User-Agent": "Mozilla/5.0 (compatible; StockFeedBot/1.0)"}
            resp = requests.get(url, headers=headers, timeout=10)
            resp.raise_for_status()
            
            root = ET.fromstring(resp.content)
            channel = root.find("channel")
            if channel is None:
                continue
            
            for item in channel.findall("item"):
                title_el = item.find("title")
                link_el = item.find("link")
                pub_el = item.find("published") or item.find("pubDate")
                desc_el = item.find("description")
                
                title = (title_el.text or "").strip() if title_el is not None else ""
                link = (link_el.text or "").strip() if link_el is not None else ""
                pub = (pub_el.text or "").strip() if pub_el is not None else ""
                desc = (desc_el.text or "").strip() if desc_el is not None else ""
                
                if not title or not link:
                    continue
                
                # ì‹œê°„ íŒŒì‹±
                try:
                    dt = parsedate_to_datetime(pub)
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=ZoneInfo("UTC"))
                    dt_kst = dt.astimezone(tz)
                    
                    if dt_kst < cutoff_time:
                        continue
                except Exception:
                    continue
                
                # í‹°ì»¤ ì¶”ì¶œ
                related_tickers = extract_tickers(title + " " + desc)
                
                # ì•„ì´í…œ ìƒì„±
                feed_item = {
                    "id": generate_item_id(link, dt_kst.isoformat()),
                    "timestamp": dt_kst.isoformat(),
                    "source_type": "SNS",
                    "source_name": "Reddit",
                    "category": "MARKET",
                    "related_tickers": related_tickers,
                    "content": title + (" - " + desc[:200] if desc else ""),
                    "url": link,
                    "sentiment": None,
                }
                
                feed_item["category"] = categorize_item(feed_item)
                feed_item["sentiment"] = determine_sentiment(feed_item["content"])
                
                all_items.append(feed_item)
        
        except Exception as e:
            print(f"[WARN] Reddit ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")
            continue
    
    return all_items


def collect_sofi_specific_sources() -> List[Dict]:
    """SoFi ì „ìš© ì†ŒìŠ¤ì—ì„œ ì½˜í…ì¸  ìˆ˜ì§‘ (Seeking Alpha ì œì™¸)"""
    all_items = []
    tz = ZoneInfo("Asia/Seoul")
    cutoff_time = datetime.now(tz) - timedelta(hours=24)
    
    # Seeking AlphaëŠ” ìŠ¤ìº  ê¸€ë“¤ì´ ë§ê³  ì¶”ì¶œë„ ì‹¤íŒ¨í•˜ë¯€ë¡œ ì œì™¸
    
    # 1. Yahoo Finance - SoFi ë‰´ìŠ¤
    yahoo_url = "https://feeds.finance.yahoo.com/rss/2.0/headline?s=SOFI&region=US&lang=en-US"
    try:
        headers = {"User-Agent": "Mozilla/5.0 (compatible; StockFeedBot/1.0)"}
        resp = requests.get(yahoo_url, headers=headers, timeout=10)
        resp.raise_for_status()
        
        root = ET.fromstring(resp.content)
        channel = root.find("channel")
        
        if channel is not None:
            for item in channel.findall("item")[:20]:
                title_el = item.find("title")
                link_el = item.find("link")
                pub_el = item.find("pubDate")
                desc_el = item.find("description")
                
                title = (title_el.text or "").strip() if title_el is not None else ""
                link = (link_el.text or "").strip() if link_el is not None else ""
                pub = (pub_el.text or "").strip() if pub_el is not None else ""
                desc = (desc_el.text or "").strip() if desc_el is not None else ""
                
                if not title or not link:
                    continue
                
                try:
                    dt = parsedate_to_datetime(pub)
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=ZoneInfo("UTC"))
                    dt_kst = dt.astimezone(tz)
                    
                    if dt_kst < cutoff_time:
                        continue
                except Exception:
                    dt_kst = datetime.now(tz)
                
                feed_item = {
                    "id": generate_item_id(link, dt_kst.isoformat()),
                    "timestamp": dt_kst.isoformat(),
                    "source_type": "NEWS",
                    "source_name": "Yahoo Finance",
                    "category": "WATCHLIST",
                    "related_tickers": ["SOFI"],
                    "content": title + (" - " + desc[:200] if desc else ""),
                    "url": link,
                    "sentiment": determine_sentiment(title + " " + desc),
                }
                
                all_items.append(feed_item)
    
    except Exception as e:
        print(f"[WARN] Yahoo Finance ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # 3. Reddit - SoFi ì „ìš© ì„œë¸Œë ˆë”§
    sofi_reddit_feeds = [
        "https://www.reddit.com/r/sofistock/hot/.rss",
        "https://www.reddit.com/r/sofi/hot/.rss",
    ]
    
    for url in sofi_reddit_feeds:
        try:
            headers = {"User-Agent": "Mozilla/5.0 (compatible; StockFeedBot/1.0)"}
            resp = requests.get(url, headers=headers, timeout=10)
            resp.raise_for_status()
            
            root = ET.fromstring(resp.content)
            channel = root.find("channel")
            if channel is None:
                continue
            
            for item in channel.findall("item")[:15]:
                title_el = item.find("title")
                link_el = item.find("link")
                pub_el = item.find("published") or item.find("pubDate")
                desc_el = item.find("description")
                
                title = (title_el.text or "").strip() if title_el is not None else ""
                link = (link_el.text or "").strip() if link_el is not None else ""
                pub = (pub_el.text or "").strip() if pub_el is not None else ""
                desc = (desc_el.text or "").strip() if desc_el is not None else ""
                
                if not title or not link:
                    continue
                
                try:
                    dt = parsedate_to_datetime(pub)
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=ZoneInfo("UTC"))
                    dt_kst = dt.astimezone(tz)
                    
                    if dt_kst < cutoff_time:
                        continue
                except Exception:
                    continue
                
                feed_item = {
                    "id": generate_item_id(link, dt_kst.isoformat()),
                    "timestamp": dt_kst.isoformat(),
                    "source_type": "SNS",
                    "source_name": "Reddit",
                    "category": "WATCHLIST",
                    "related_tickers": ["SOFI"],
                    "content": title + (" - " + desc[:200] if desc else ""),
                    "url": link,
                    "sentiment": determine_sentiment(title + " " + desc),
                }
                
                all_items.append(feed_item)
        
        except Exception as e:
            print(f"[WARN] Reddit SoFi ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")
            continue
    
    return all_items


def load_existing_feed() -> List[Dict]:
    """ê¸°ì¡´ stock_feed.json ë¡œë“œ"""
    if not STOCK_FEED_JSON_PATH.exists():
        return []
    
    try:
        with open(STOCK_FEED_JSON_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("items", [])
    except Exception as e:
        print(f"[WARN] ê¸°ì¡´ í”¼ë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []


def merge_items(existing: List[Dict], new_items: List[Dict]) -> List[Dict]:
    """ê¸°ì¡´ ì•„ì´í…œê³¼ ìƒˆ ì•„ì´í…œ ë³‘í•© (ì¤‘ë³µ ì œê±°)"""
    existing_ids: Set[str] = {item["id"] for item in existing}
    
    # ìƒˆ ì•„ì´í…œ ì¤‘ ì¤‘ë³µ ì œê±°
    unique_new = [item for item in new_items if item["id"] not in existing_ids]
    
    # ë³‘í•© ë° ì •ë ¬ (ìµœì‹ ìˆœ)
    all_items = existing + unique_new
    all_items.sort(key=lambda x: x["timestamp"], reverse=True)
    
    # ìƒìœ„ 200ê°œë§Œ ìœ ì§€
    return all_items[:MAX_ITEMS]


def save_feed(items: List[Dict]):
    """stock_feed.json ì €ì¥"""
    tz = ZoneInfo("Asia/Seoul")
    data = {
        "last_updated": datetime.now(tz).isoformat(),
        "items": items,
    }
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    STOCK_FEED_JSON_PATH.parent.mkdir(parents=True, exist_ok=True)
    
    with open(STOCK_FEED_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"[OK] {len(items)}ê°œ ì•„ì´í…œ ì €ì¥ ì™„ë£Œ: {STOCK_FEED_JSON_PATH}")


def send_sofi_discord_notification(webhook_url: str, items: List[Dict]) -> int:
    """SOFI ê´€ë ¨ ìƒˆ ë‰´ìŠ¤ë¥¼ Discordë¡œ ì „ì†¡"""
    if not webhook_url:
        return 0
    
    import requests
    from datetime import datetime
    
    sent_count = 0
    
    for item in items:
        # SOFI ê´€ë ¨ ì•„ì´í…œë§Œ í•„í„°ë§
        tickers = item.get("related_tickers", [])
        if "SOFI" not in tickers:
            continue
        
        # Discord Embed ìƒì„±
        embed = {
            "title": f"ğŸ“° SOFI ì†Œì‹: {item.get('content', '')[:100]}",
            "description": item.get("content", "")[:500],
            "color": 0x00FF00 if item.get("sentiment") == "POSITIVE" else (0xFF0000 if item.get("sentiment") == "NEGATIVE" else 0x5865F2),
            "timestamp": item.get("timestamp", datetime.utcnow().isoformat()),
            "fields": [
                {
                    "name": "ì¶œì²˜",
                    "value": f"{item.get('source_name', 'Unknown')} ({item.get('source_type', 'NEWS')})",
                    "inline": True,
                },
                {
                    "name": "ì¹´í…Œê³ ë¦¬",
                    "value": item.get("category", "MARKET"),
                    "inline": True,
                },
            ],
        }
        
        if item.get("sentiment"):
            embed["fields"].append({
                "name": "ê°ì • ë¶„ì„",
                "value": item.get("sentiment", "NEUTRAL"),
                "inline": True,
            })
        
        if item.get("url"):
            embed["url"] = item["url"]
        
        embed["footer"] = {"text": "Stock Feed Agent"}
        
        payload = {"embeds": [embed]}
        
        try:
            response = requests.post(webhook_url.strip().strip('"').strip("'"), json=payload, timeout=10)
            response.raise_for_status()
            sent_count += 1
            print(f"[OK] Discord ì•Œë¦¼ ì „ì†¡: {item.get('content', '')[:50]}...")
        except Exception as e:
            print(f"[WARN] Discord ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    return sent_count


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import os
    
    print("[INFO] ì£¼ì‹ ë‰´ìŠ¤ í”¼ë“œ ìˆ˜ì§‘ ì‹œì‘...")
    
    # 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    existing_items = load_existing_feed()
    existing_ids = {item["id"] for item in existing_items}
    print(f"[INFO] ê¸°ì¡´ ì•„ì´í…œ: {len(existing_items)}ê°œ")
    
    # 2. ìƒˆ ë°ì´í„° ìˆ˜ì§‘
    print("[INFO] RSS ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    news_items = collect_rss_news()
    print(f"[INFO] RSS ë‰´ìŠ¤: {len(news_items)}ê°œ")
    
    print("[INFO] Reddit ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì¤‘...")
    reddit_items = collect_reddit_posts()
    print(f"[INFO] Reddit ê²Œì‹œë¬¼: {len(reddit_items)}ê°œ")
    
    print("[INFO] SoFi ì „ìš© ì†ŒìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    sofi_items = collect_sofi_specific_sources()
    print(f"[INFO] SoFi ì „ìš© ì†ŒìŠ¤: {len(sofi_items)}ê°œ")
    
    # 3. ë³‘í•©
    all_new_items = news_items + reddit_items + sofi_items
    
    # 4. SOFI ê´€ë ¨ ìƒˆ ë‰´ìŠ¤ë§Œ í•„í„°ë§ (Discord ì•Œë¦¼ìš©)
    sofi_new_items = [
        item for item in all_new_items
        if item["id"] not in existing_ids and "SOFI" in item.get("related_tickers", [])
    ]
    
    # 5. Discord ì•Œë¦¼ ì „ì†¡ (SOFI ê´€ë ¨ ìƒˆ ë‰´ìŠ¤ë§Œ, ì¤‘ìš” ë‰´ìŠ¤ë§Œ í•„í„°ë§)
    discord_webhook = os.getenv("DISCORD_WEBHOOK_URL")
    if sofi_new_items and discord_webhook:
        # ì¤‘ìš” ë‰´ìŠ¤ë§Œ í•„í„°ë§ (ì œëª©ì— íŠ¹ì • í‚¤ì›Œë“œê°€ ìˆê±°ë‚˜, íŠ¹ì • ì†ŒìŠ¤ì¸ ê²½ìš°)
        important_keywords = ["earnings", "ì‹¤ì ", "ë¶„ê¸°", "quarter", "guidance", "ê°€ì´ë˜ìŠ¤", 
                             "acquisition", "ì¸ìˆ˜", "merger", "í•©ë³‘", "partnership", "ì œíœ´",
                             "regulation", "ê·œì œ", "approval", "ìŠ¹ì¸", "launch", "ì¶œì‹œ"]
        important_sources = ["Yahoo Finance", "Bloomberg"]
        
        important_items = [
            item for item in sofi_new_items
            if any(keyword.lower() in item.get("content", "").lower() for keyword in important_keywords)
            or item.get("source_name") in important_sources
        ]
        
        # ì¤‘ìš” ë‰´ìŠ¤ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒë§Œ, ì—†ìœ¼ë©´ ì „ì²´ ì „ì†¡ (ìµœëŒ€ 5ê°œë¡œ ì œí•œ)
        items_to_notify = important_items[:5] if important_items else sofi_new_items[:3]
        
        if items_to_notify:
            print(f"[INFO] SOFI ê´€ë ¨ ì¤‘ìš” ë‰´ìŠ¤ {len(items_to_notify)}ê°œ ë°œê²¬, Discord ì•Œë¦¼ ì „ì†¡ ì¤‘...")
            sent_count = send_sofi_discord_notification(discord_webhook, items_to_notify)
            print(f"[OK] Discord ì•Œë¦¼ {sent_count}ê°œ ì „ì†¡ ì™„ë£Œ")
        else:
            print(f"[INFO] SOFI ê´€ë ¨ ìƒˆ ë‰´ìŠ¤ {len(sofi_new_items)}ê°œ ë°œê²¬í–ˆì§€ë§Œ ì¤‘ìš” ë‰´ìŠ¤ëŠ” ì—†ìŠµë‹ˆë‹¤.")
    elif sofi_new_items:
        print(f"[INFO] SOFI ê´€ë ¨ ìƒˆ ë‰´ìŠ¤ {len(sofi_new_items)}ê°œ ë°œê²¬ (Discord ì•Œë¦¼ ë¯¸ì„¤ì •)")
    
    # 6. ë³‘í•© ë° ì €ì¥
    merged_items = merge_items(existing_items, all_new_items)
    print(f"[INFO] ë³‘í•© í›„ ì´ ì•„ì´í…œ: {len(merged_items)}ê°œ")
    
    # 7. ì €ì¥
    save_feed(merged_items)
    print("[OK] ì™„ë£Œ!")


if __name__ == "__main__":
    main()
